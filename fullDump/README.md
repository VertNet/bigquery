fullDump
========

Migration of a snapshot of the VertNet harvested files to BigQuery.

Requirements
------------

In order to run, this script needs the googleapis module installed and accessible to python via the PYTHONPATH. The module and instructions on how to install it can be found here: https://github.com/jotegui/googleapis.

It also requires the client_secrets.json file, which can be obtained through the Developers' console (https://console.developers.google.com). Please refer to the documentation in the googleapis GitHub repo for instructions on how to get this client_secrets.json file.

The configuration variables for the BigQuery and CloudStorage services are included in the folder, in the fullDump_cred.py module. The fullDump.py script is prepared to load them automatically.

The load jobs performed by BigQuery require that a schema of the table be specified. This must be provided in the schema.json file, which _must_ be in the same folder as fullDump.py. The repository already has a working example of such file.

Setup
-----

The first run of the script will generate two files: bigquery.dat and storage.dat. These files avoid having to re-authenticate each time the script is run.

Although not necessary, these files can be pre-built. This, in fact, can help making sure the googleapis module is accessible to python. To do that, open the python interpreter and execute the following code:

    from googleapis import BigQuery, CloudStorage  # Import the module
    from fulldump_cred import cs_cred, bq_cred  # Import the credentials
    bq = BigQuery.BigQuery(bq_cred)
    cs = CloudStorage.CloudStorage(cs_cred)

The authentication flow will run twice (_i.e._, the authentication window will open twice), one for BigQuery and another one for CloudStorage. Keep the client_secrets.json file and the two .dat files in each folder that will run any googleapi services; that way, you won't need to re-authenticate.

The schema.json file can be generated by running the build_schema.py program, just like this:

    $ ./build_schema.py

This only needs a file named harvest_fields with a list of all the names of the fields in the harvested files, one name per line, with nothing more. It will overwrite any existing schema.json file, so make sure you create a backup, just in case.

The number of fields _must match_ between the schema.json file and the data files, or the load jobs will fail.

How it works
------------

Make sure the client_secrets.json, schema.json, bigquery.dat and storage.dat files are in the same folder as the fullDump.py and fulldump_cred.py files.

The script is prepared to build a new BigQuery table in the 'dumps' dataset, called full_YYYYMMDD, with the current date in the specified format. For example, on July 30th, 2014, it would create the table dumps.full_20140730. The default values can be overriden by changing the appropriate attributes in the fullDump.py, lines 21-23. Besides, the script will generate a file for the failed uploads (see below), named 'failed.txt' by default. This can also be changed, on line 26.

You can customize how the harvest folders are selected from the CartoDB file by editing the SELECT statments in fullDump.py ~l43.

Then simply run:

    python fullDump.py

And enjoy the show. The program will throw updates on each step.

At the end, the program will inform if there was any file that could not be loaded into BigQuery. The list of failed files will be located in the same folder as fullDump.py, and will be called by default 'failed.txt'. These failures usually happen because there is a strange character in the data files, and these files cannot be uploaded to BigQuery unless these characters have been dealt with.

What it does
------------

This program dumps a full snapshot of the VertNet harvested files into a new BigQuery table.

1. It first creates the 'dumps' dataset, if it doesn't previously exist.
2. Then, it extracts a list of all the VertNet IPT resources and their last harvested folder path from the CartoDB resource_staging table. Important: Make sure the the harvestfolder field is filled with the latest harvest for the resource. Even more important: Make sure that the schema for every harvested file is consistent with the schema for BigQuery. If there was a change in the harvesting schema, there will have to be a complete reharvest and full dump from these newly harvested files.
3. For each of those files, it creates a single job to load the data into the BigQuery table
4. Once all jobs have finished, it checks if any job has failed. This usually happens because a data file has a strange character. If the load job fails, the whole upload of the resource is aborted.
5. For those failed jobs, it runs an individual load job for each data file. Thus, all files except the problematic ones will be present in the dump.
6. Lastly, it dumps a list of the failed data files to a local text file, named 'failed.txt' by default.

Contact
-------

Any question, shoot me an email (javier.otegui@gmail.com)
